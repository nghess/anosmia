{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import signal\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from core import load_ephys\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the data directory and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = r\"E:\\clickbait-ephys\\data\"\n",
    "save_dir = r\"E:\\clickbait-ephys\\data\\preprocessed\"\n",
    "mice = ['6000']\n",
    "run_sessions = [14]\n",
    "session_offset = 3\n",
    "gain = 0.1949999928474426\n",
    "\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "open_ephys_dir = os.path.join(data_dir, \"open-ephys\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä Extracting OpenEphys Data (Sniff, Ephys, TTLs) and Restructuring the Data\n",
    "\n",
    "The script processes raw OpenEphys data, extracting relevant signals (LFP, MUA, sniff, TTLs) and saving them in a structured format. The preprocessed data will be stored in the `preprocessed_data` directory with the following structure:\n",
    "\n",
    "\n",
    "```\n",
    "preprocessed_data\n",
    "‚îú‚îÄ‚îÄ <animal_id>\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ <session_id>\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ lfp.npy\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ mua.npy\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sniff.npy\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ttls.npy\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "#### üóÇÔ∏è **Data Descriptions:**\n",
    "\n",
    "- **`lfp.npy`** *(Local Field Potential)*  \n",
    "  - **Description:** Represents the slow oscillatory activity of the brain.  \n",
    "  - **Processing:** Low-pass filtered at **300 Hz** to capture relevant neural dynamics.\n",
    "\n",
    "- **`mua.npy`** *(Multi-Unit Activity)*  \n",
    "  - **Description:** Captures high-frequency spiking activity from multiple neurons.  \n",
    "  - **Processing:** Band-pass filtered between **300 Hz and 6 kHz** and **median common average referenced** within regions to reduce noise.\n",
    "\n",
    "- **`sniff.npy`** *(Sniff Data)*  \n",
    "  - **Description:** Raw sniffing signal representing respiratory behavior.  \n",
    "  - **Processing:** Extracted directly from the raw data without additional filtering.\n",
    "\n",
    "- **`ttls.npy`** *(TTL Pulses)*  \n",
    "  - **Description:** Raw TTL (Transistor-Transistor Logic) pulses used as event markers.  \n",
    "  - **Processing:** Saved in raw format to preserve precise event timing.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all files in the open-ephys directory\n",
    "all_files = os.listdir(open_ephys_dir)\n",
    "\n",
    "# Loop through each mouse\n",
    "for mouse in mice:\n",
    "    # Filter files related to the current mouse\n",
    "    mouse_sessions = [f.strip() for f in all_files if mouse in f]\n",
    "\n",
    "    # Process each session for the current mouse\n",
    "    for session in tqdm(mouse_sessions):\n",
    "        print(f\"Processing session: {session}\")\n",
    "\n",
    "        # Extract date and session number\n",
    "        date = session[:10]\n",
    "        try:\n",
    "            session_num = int(date[-2:]) - session_offset\n",
    "        except ValueError:\n",
    "            print(f\"Invalid date format in session: {session}\")\n",
    "            continue\n",
    "\n",
    "        if session_num not in run_sessions:\n",
    "            print(f\"Skipping session {session_num}\")\n",
    "            continue\n",
    "        \n",
    "        # Build the path to the data directory\n",
    "        session_path = os.path.join(open_ephys_dir, session)\n",
    "\n",
    "        # Check if the session directory exists\n",
    "        if not os.path.isdir(session_path):\n",
    "            print(f\"Session directory not found: {session_path}\")\n",
    "            continue\n",
    "\n",
    "        # Get the first subdirectory (assumed to be the \"Record Node\" folder)\n",
    "        try:\n",
    "            record_node = os.listdir(session_path)[0]\n",
    "        except IndexError:\n",
    "            print(f\"No files found in session: {session_path}\")\n",
    "            continue\n",
    "\n",
    "        # Build the path to the continuous data directory\n",
    "        continuous_dir = os.path.join(session_path, record_node, 'experiment2', 'recording1', 'continuous')\n",
    "\n",
    "        # Check if the continuous directory exists\n",
    "        if not os.path.isdir(continuous_dir):\n",
    "            print(f\"Continuous directory not found: {continuous_dir}\")\n",
    "            continue\n",
    "\n",
    "        # Get the acquisition board folder\n",
    "        try:\n",
    "            acquisition_board = os.listdir(continuous_dir)[0]\n",
    "        except IndexError:\n",
    "            print(f\"No acquisition board folder found in: {continuous_dir}\")\n",
    "            continue\n",
    "\n",
    "        # Final path to the continuous.dat file\n",
    "        data_path = os.path.join(continuous_dir, acquisition_board, 'continuous.dat')\n",
    "        print(f\"Data path: {data_path}\")\n",
    "\n",
    "        # Load the electrophysiological data\n",
    "        if os.path.exists(data_path):\n",
    "            try:\n",
    "\n",
    "                # Making the data directory\n",
    "                data_dir = os.path.join(save_dir, mouse, str(session_num))\n",
    "                os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "                # Ensuring files do not already exist\n",
    "                if os.path.exists(os.path.join(data_dir, 'lfp.npy')) or \\\n",
    "                   os.path.exists(os.path.join(data_dir, 'mua.npy')) or \\\n",
    "                   os.path.exists(os.path.join(data_dir, 'sniff.npy')) or \\\n",
    "                   os.path.exists(os.path.join(data_dir, 'ttls.npy')):\n",
    "                    print(f\"Files already exist for {session}, skipping...\")\n",
    "                    continue\n",
    "\n",
    "                # Loading the data\n",
    "                data = load_ephys(data_path, nchannels=40, dtype=np.int16, order='F')\n",
    "\n",
    "\n",
    "                # Extracting the ephys, sniff, and ttl timeseries\n",
    "                ephys = (data[:32, :] * gain).astype(np.float64)\n",
    "                sniff = data[-1, :]\n",
    "                ttls = data[-2, :]\n",
    "\n",
    "                # lowpass filtering the ephys data to get the LFP\n",
    "                sos = signal.butter(4, 300, 'lowpass', fs=30000, output='sos')\n",
    "                lfps = signal.sosfiltfilt(sos, ephys)\n",
    "\n",
    "                # bandpass filtering the ephys data to get the MUA\n",
    "                sos = signal.butter(4, [300, 6000], 'bandpass', fs=30000, output='sos')\n",
    "                mua = signal.sosfiltfilt(sos, ephys)\n",
    "\n",
    "                # median common average referencing the multiunit activity\n",
    "                mua[:16, :] -= np.median(mua[:16, :], axis=0)\n",
    "                mua[16:, :] -= np.median(mua[16:, :], axis=0)\n",
    "\n",
    "                # converting back to int16\n",
    "                lfps = lfps.astype(np.int16)\n",
    "                mua = mua.astype(np.int16)\n",
    "                sniff = sniff.astype(np.int16)\n",
    "                ttls = ttls.astype(np.int16)\n",
    "\n",
    "                \n",
    "\n",
    "                # Saving the data as binary files\n",
    "                np.save(os.path.join(data_dir, 'lfp.npy'), lfps)\n",
    "                np.save(os.path.join(data_dir, 'mua.npy'), mua)\n",
    "                np.save(os.path.join(data_dir, 'sniff.npy'), sniff)\n",
    "                np.save(os.path.join(data_dir, 'ttls.npy'), ttls)\n",
    "\n",
    "                print(f\"Loaded data successfully for {session}\\n\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading data from {data_path}: {e}\")\n",
    "        else:\n",
    "            print(f\"Data path not found: {data_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_sessions = ['14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24']\n",
    "mice = ['6001', '6002', '6003']\n",
    "\n",
    "# loop through the preprocessed data and donwsample the lfp files to 1Khz\n",
    "for mouse in mice:\n",
    "    mouse_dir = os.path.join(save_dir, mouse)\n",
    "    for session in tqdm(os.listdir(mouse_dir), desc=f\"Processing {mouse}\"):\n",
    "\n",
    "        if session not in run_sessions:\n",
    "            print(f\"Skipping session {session}\")\n",
    "            continue\n",
    "\n",
    "        session_dir = os.path.join(mouse_dir, session)\n",
    "        lfp_path = os.path.join(session_dir, 'lfp.npy')\n",
    "        if os.path.exists(lfp_path):\n",
    "            print(f\"Downsampling LFP for {session}\")\n",
    "            lfp = np.load(lfp_path)\n",
    "\n",
    "            # lowpass filter the lfp to 300Hz\n",
    "            sos = signal.butter(4, 300, 'lowpass', fs=1000, output='sos')\n",
    "            lfp = signal.sosfiltfilt(sos, lfp)\n",
    "            \n",
    "            # downsample the lfp to 1kHz\n",
    "            lfp = signal.decimate(lfp, 30, axis=1)\n",
    "            np.save(lfp_path, lfp)\n",
    "        else:\n",
    "            print(f\"LFP file not found for {session}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### helping kilosort amandas data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate binary files \n",
    "files = [r\"C:\\Users\\smearlab\\clickbait-ephys\\DOI\\5001\\050924_5001_DOI 1_Ephys.bin\"]\n",
    "\n",
    "concatenated_data = []\n",
    "for file in files:\n",
    "    print(f\"Loading {file}...\")\n",
    "    data = load_ephys(file, nchannels=32, dtype=np.uint16, order='F') # Notice the data is stored as uint16\n",
    "\n",
    "    # bandpass filtering the ephys data to get the MUA\n",
    "    print(f\"Filtering {file}\")\n",
    "    sos = signal.butter(4, [300, 6000], 'bandpass', fs=30000, output='sos')\n",
    "    data = signal.sosfiltfilt(sos, data)\n",
    "\n",
    "    # median common average referencing the multiunit activity\n",
    "    print(f\"Median referencing {file}\")\n",
    "    data -= np.median(data, axis=0)\n",
    "\n",
    "    # convert back to int16\n",
    "    data = data.astype(np.int16)\n",
    "\n",
    "    print('plotting')\n",
    "    plt.figure(figsize=(15, 7))\n",
    "    plt.plot(data[0, :30_000])\n",
    "    plt.show()\n",
    "    concatenated_data.append(data)\n",
    "    del data\n",
    "print('concatenating')\n",
    "concatenated_data = np.concatenate(concatenated_data, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# reshape the concatenated data for kilosort\n",
    "print('reshaping')\n",
    "concatenated_data = concatenated_data.reshape(-1, order = 'F')\n",
    "\n",
    "# save concatenated data to a new file\n",
    "print('saving')\n",
    "output_file = r\"C:\\Users\\smearlab\\clickbait-ephys\\DOI\\5001\\concatenated_data_bandpass.bin\"\n",
    "with open(output_file, 'wb') as f:\n",
    "    concatenated_data.tofile(f)\n",
    "    \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
