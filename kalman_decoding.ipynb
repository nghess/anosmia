{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kalman filter for decoding position from population firing rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose $x_t = \\begin{pmatrix} x \\\\ y \\\\ v_x \\\\ v_y \\end{pmatrix}$ is the latent state (position and velocity) of the mouse in a 2D envirnment, and $y_t$ is the firing rate of a population of neurons. The Kalman filter can be used to decode the position from the firing rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we assume that $x_t$ evolves according to a linear dynamical state space model with Gaussian noise:\n",
    "$$\n",
    "x_t = A x_{t-1} + w_t, \\quad w_t \\sim \\mathcal{N}(0, Q)$$\n",
    "where $A$ is the state transition matrix and $Q$ is the process noise covariance matrix. For example, the state transition matrix ccould be written as:\n",
    "$$\n",
    "A = \\begin{pmatrix} 1 & 0 & \\Delta t & 0 \\\\ 0 & 1 & 0 & \\Delta t \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 1 \\end{pmatrix}$$\n",
    "where $\\Delta t$ is the time step. In our case we fit the model to the data, so we don't need to specify $A$ and $Q$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We assume that the firing rates are related to the latent state by a linear observation model:\n",
    "$$\n",
    "y_t = H x_t + v_t, \\quad v_t \\sim \\mathcal{N}(0, R)$$\n",
    "where $H$ is the observation matrix and $R$ is the observation noise covariance matrix. The observation matrix can be fit to the firing rates of the neurons in the least squares sense:\n",
    "$$\n",
    "\\min_{H} \\sum_{t=1}^T \\| y_t - H x_t \\|^2$$\n",
    "where $T$ is the number of time steps. This is a standard linear regression problem, and the solution can be obtained by:\n",
    "$$\n",
    "H = \\bold{Y}\\bold{X}^T(\\bold{X}\\bold{X}^T)^{-1}$$\n",
    "where $\\bold{Y}$ is the matrix of firing rates and $\\bold{X}$ is the matrix of latent states. We also fit the process noise covariance matrix $Q$ and the observation noise covariance matrix $R$ using the minimum mean-square error estimates:\n",
    "$$\n",
    "Q = \\frac{1}{T} \\sum_{t=1}^T (x_t - A x_{t-1})(x_t - A x_{t-1})^T$$\n",
    "$$\n",
    "R = \\frac{1}{T} \\sum_{t=1}^T (y_t - H x_t)(y_t - H x_t)^T$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will introduce the estimate $\\hat{x}_t$ of the latent state at time $t$. We say that $\\hat{x}_t$ comes from a similar linear dynamical system:\n",
    "$$\n",
    "\\hat{x}_t = A \\hat{x}_{t-1} + L_t (y_t - H \\hat{x}_{t-1})$$\n",
    "where $L_t (y_t - H \\hat{x}_{t-1})$ is feedback proportional to the difference between observed and predicted outputs. We wish to find $\\hat{x}_t$ that minimizes the mean squared error:\n",
    "$$\n",
    "P_k = \\mathbb{E}[(x_t - \\hat{x}_t)(x_t - \\hat{x}_t)^T]$$\n",
    "This is a standard Kalman filter problem (Kalman, 1961), and the solution is given by:\n",
    "$$\n",
    "L_t = A P_t H^T (R + H P_t C^T H)^{-1}$$\n",
    "where $P_t$ is defined recursively as:\n",
    "$$\n",
    "P_t = (A - L_t C)P_{t-1} (A - L_t C)^T + Q + L_t R L_t^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The Kalman filter can then be used to decode the position from the firing rates in heald out data. We first initialize the state estimate $\\hat{x}_0 \\sim \\mathcal{N}(0, P_0)$, where $P_0$ is the initial state covariance matrix. Following (Malik et al., 2011) we chose $x_0 = 0$ and $P_0 = Q$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from decoding_library import *\n",
    "from core import compute_spike_latency, compute_spike_rates_sliding_window_by_region_smooth, load_behavior, align_brain_and_behavior\n",
    "from plotting import plot_spike_rates, plot_position_trajectories\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the neural data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the kilosorted data directory\n",
    "kilosort_dir = r\"E:\\clickbait-ephys\\data\\kilosorted_data_20um_tetrode\\6002\\7\"\n",
    "\n",
    "coding_type = 'latency'\n",
    "fs = 30000 # Hz\n",
    "\n",
    "if coding_type == 'rate':\n",
    "    # Parameters for building the spike rate matrix\n",
    "    window_size = .1 # seconds\n",
    "    step_size = .1 # seconds\n",
    "    smoothing_size = 0.5 #bins\n",
    "\n",
    "    rates_OB, rates_HC, time_bins, units_OB, units_HC = compute_spike_rates_sliding_window_by_region_smooth(kilosort_dir, fs, window_size, step_size, use_units='good/mua', sigma=2.5, zscore=True)\n",
    "    plot_spike_rates(time_bins, rates_OB, rates_HC, units_OB, units_HC, dark_mode=True, global_font=\"Arial\", show=True, global_font_size=20, normalized = True, cmap = 'Electric')\n",
    "\n",
    "elif coding_type == 'latency':\n",
    "    window_size = 30 # samples\n",
    "    print('Computing spike latencies...')\n",
    "    spike_latency_matrix_OB, spike_latency_matrix_HC, time_bins, ob_units, hc_units = compute_spike_latency(kilosort_dir, fs, window_size, use_units='good/mua')\n",
    "    timestamps = np.arange(0, spike_latency_matrix_OB.shape[1], 1)/fs\n",
    "    print('Normaling spike latencies...')\n",
    "    normalized_latency_matrix_OB = prepare_latencies_for_kalman(spike_latency_matrix_OB)\n",
    "    del spike_latency_matrix_OB\n",
    "    #normalized_latency_matrix_HC = prepare_latencies_for_kalman(spike_latency_matrix_HC)\n",
    "    #del spike_latency_matrix_HC\n",
    "print(time_bins[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the events file which contains the tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specifying the behavior directory\n",
    "behavior_dir = r\"E:\\clickbait-ephys\\data\\behavior_data\\6002\\7\"\n",
    "\n",
    "behavior = load_behavior(behavior_dir)\n",
    "\n",
    "if coding_type == 'rate':\n",
    "    data = align_brain_and_behavior(behavior, rates_HC, units_HC, time_bins, window_size, speed_threshold = 3)\n",
    "elif coding_type == 'latency':\n",
    "    data = align_brain_and_behavior(behavior, normalized_latency_matrix_OB, hc_units, timestamps, speed_threshold = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the position over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('dark_background')\n",
    "plot_position_trajectories(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the data into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "behavior = data[['x', 'y', 'v_x', 'v_y']].values\n",
    "spike_rates = data.iloc[:, :-6].values\n",
    "rates_train, rates_test, switch_ind = cv_split(spike_rates, 2, k_CV=10, n_blocks=10)\n",
    "behavior_train, behavior_test, _ = cv_split(behavior, 2, k_CV=10, n_blocks=10)\n",
    "print(\"Behavior train shape:\", behavior_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the model\n",
    "kf = KalmanFilterDecoder(state_dim = behavior_test.shape[1], lambda_reg = 1e-5)\n",
    "kf.train(behavior_train, rates_train)\n",
    "\n",
    "\n",
    "# Decode test data\n",
    "x_est, P = kf.decode(rates_test, switch_indices=switch_ind, gain = 1)\n",
    "\n",
    "# Plot results\n",
    "kf.plot_results(x_est, true_behavior=behavior_test, switch_indices=switch_ind)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FULL ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ranksums\n",
    "\n",
    "\n",
    "\n",
    "def process_session(params):\n",
    "\n",
    "    # Unpack the parameters\n",
    "    mouse, session, kilosort_dir, behavior_dir, save_dir, window_size, step_size, fs, sigma_smooth, use_units, speed_threshold, k_CV, n_blocks, n_shifts, plot_predictions = params\n",
    "\n",
    "    try:\n",
    "\n",
    "        # Load the spike rates and behavior data\n",
    "        rates_OB, rates_HC, time_bins, units_OB, units_HC = compute_spike_rates_sliding_window_by_region_smooth(kilosort_dir, fs, window_size, step_size, use_units=use_units, sigma=sigma_smooth, zscore=True)\n",
    "        events = load_behavior(behavior_dir)\n",
    "\n",
    "        # Looping through the regions\n",
    "        for region in ['OB', 'HC']:\n",
    "            if region == 'OB':\n",
    "                rates = rates_OB\n",
    "                units = units_OB\n",
    "            else:\n",
    "                rates = rates_HC\n",
    "                units = units_HC\n",
    "            region_save_path = os.path.join(save_dir, mouse, session, region)\n",
    "            os.makedirs(region_save_path, exist_ok=True)\n",
    "\n",
    "            # Aligning the brain and behavior data\n",
    "            data = align_brain_and_behavior(events, rates, units, time_bins, window_size, speed_threshold = speed_threshold)\n",
    "            plot_position_trajectories(data, save_path=region_save_path)\n",
    "\n",
    "            spike_rates = data.iloc[:, :-6].values\n",
    "\n",
    "            # Unpack the results\n",
    "            true_rmse = []\n",
    "            null_rmse = []\n",
    "\n",
    "            # Loop through the shifts\n",
    "            for shift in range(n_shifts + 1):\n",
    "                if shift == 0:\n",
    "                    behavior = data[['x', 'y', 'v_x', 'v_y']].values\n",
    "                else:\n",
    "                    roll_value = np.random.randint(100, len(data) - 100)\n",
    "                    behavior = data[['x', 'y', 'v_x', 'v_y']].values.copy()\n",
    "                    behavior = np.roll(behavior, roll_value, axis=0)\n",
    "\n",
    "                for k in range(k_CV):\n",
    "                    rates_train, rates_test, switch_ind = cv_split(spike_rates, k, k_CV=k_CV, n_blocks=n_blocks)\n",
    "                    behavior_train, behavior_test, _ = cv_split(behavior, k, k_CV=k_CV, n_blocks=n_blocks)\n",
    "\n",
    "                    # Create and train the model\n",
    "                    kf = KalmanFilterDecoder(state_dim=behavior_test.shape[1], lambda_reg=1e-5)\n",
    "                    kf.train(behavior_train, rates_train)\n",
    "                    \n",
    "                    # Decode test data\n",
    "                    x_est, P = kf.decode(rates_test, switch_indices=switch_ind, gain=1)\n",
    "\n",
    "                    # Plot results\n",
    "                    if plot_predictions:\n",
    "                        kf.plot_results(x_est, true_behavior=behavior_test, switch_indices=switch_ind)\n",
    "                        predictions_save_path = os.path.join(region_save_path, \"predictions\")\n",
    "                        os.makedirs(predictions_save_path, exist_ok=True)\n",
    "                        plt.savefig(os.path.join(predictions_save_path, f\"shift_{shift}_k_{k}.png\"), dpi=300)\n",
    "\n",
    "\n",
    "                    # Calculate Euclidean distance error at each time point\n",
    "                    euclidean_distances = np.sqrt(np.sum((x_est[:, :2] - behavior_test[:, :2])**2, axis=1))\n",
    "\n",
    "                    # Calculate RMSE of these distances\n",
    "                    rmse = np.sqrt(np.mean(euclidean_distances**2))\n",
    "\n",
    "                    # Store the MSE\n",
    "                    if shift == 0:\n",
    "                        true_rmse.append(rmse)\n",
    "                    else:\n",
    "                        null_rmse.append(rmse)\n",
    "\n",
    "            true_rmse = np.array(true_rmse)\n",
    "            null_rmse = np.array(null_rmse)\n",
    "\n",
    "            # rank sum test\n",
    "            _ , p_val = ranksums(true_rmse, null_rmse, 'less')\n",
    "            print(f\"Rank sum test p-value for {mouse}/{session}: {p_val}\")\n",
    "            \n",
    "            # plot histograms of the rmse\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.hist(true_rmse, bins=10, alpha=0.5, label='True MSE', color='blue', density=True)\n",
    "            plt.hist(null_rmse, bins=10, alpha=0.5, label='Null MSE', color='red', density=True)\n",
    "            plt.xlabel('RMSE')\n",
    "            plt.ylabel('Density')\n",
    "            plt.title(f'RMSE Distribution for {mouse}/{session}/{region}/p_val: {p_val:.2e}')\n",
    "            plt.legend()\n",
    "            plt.savefig(os.path.join(region_save_path, 'mse_distribution.png'), dpi=300)\n",
    "            plt.close()\n",
    "\n",
    "            # create a text file to save the p-value\n",
    "            with open(os.path.join(region_save_path, 'p_value.txt'), 'w') as f:\n",
    "                f.write(f\"Rank sum test p-value: {p_val}\\n\")\n",
    "\n",
    "            # save the mse results\n",
    "            np.save(os.path.join(region_save_path, f\"true_rmse.npy\"), true_rmse)\n",
    "            np.save(os.path.join(region_save_path, f\"null_rmse.npy\"), null_rmse)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing session {mouse}/{session}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Use this cell to run the analysis on a specified subset of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "\n",
    "# Set plotting style and context\n",
    "matplotlib.use('Agg')  # Use non-interactive backend to avoid issues in threaded environment\n",
    "plt.style.use('default')\n",
    "sns.set_context('poster')\n",
    "\n",
    "\n",
    "# defining directories\n",
    "spike_dir = r\"E:\\clickbait-ephys\\data\\kilosorted_data_20um_tetrode\"\n",
    "save_dir = r\"E:\\clickbait-ephys\\figures\\Kalman_filter (2-27-25)\"\n",
    "events_dir = r\"E:\\clickbait-ephys\\data\\behavior_data\"\n",
    "\n",
    "\n",
    "# defining the subset of data to process\n",
    "mice = ['6002', '6003', '6000', '6001', ]\n",
    "sessions = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14']\n",
    "\n",
    "\n",
    "# defining the data variables\n",
    "fs = 30_000  # sampling rate for neural data (mua.npy)\n",
    "sfs = 1_000  # sampling rate for sniff (sniff.npy)\n",
    "\n",
    "\n",
    "# Defining the data preprocessing parameters\n",
    "window_size = .1  # Window size for spike rate computation (in seconds)\n",
    "step_size = .1  # Step size for sliding window (in seconds)\n",
    "sigma_smooth = 2.5  # Standard deviation for gaussian smoothing of spike rates\n",
    "use_units = 'good/mua' # What kilosort cluster labels to use\n",
    "speed_threshold = 3  # Tracking point with z-scored speed above this value will be removed before interpolation\n",
    "\n",
    "\n",
    "# Defining the decoding parameters\n",
    "n_shifts = 10 # Define number of shifts for circular shifting of behavior data\n",
    "k_CV = 10 # Define number of cross-validation folds\n",
    "n_blocks = 12 # Define number of blocks for cross-validation\n",
    "plot_predictions = False\n",
    "max_workers = 6 # Define number of threads to use for parallel processing\n",
    "\n",
    "\n",
    "# creating a directory to save the figures\n",
    "save_dir = os.path.join(save_dir, f\"window_size_{window_size}_step_size_{step_size}_sigma_{sigma_smooth}_n_shifts_{n_shifts}\")\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "# saving a textfile with all parameters\n",
    "with open(os.path.join(save_dir, 'parameters.txt'), 'w') as f:\n",
    "    f.write(f\"window_size: {window_size}\\n\")\n",
    "    f.write(f\"step_size: {step_size}\\n\")\n",
    "    f.write(f\"sigma_smooth: {sigma_smooth}\\n\")\n",
    "    f.write(f\"use_units: {use_units}\\n\")\n",
    "    f.write(f\"speed_threshold: {speed_threshold}\\n\")\n",
    "    f.write(f\"n_shifts: {n_shifts}\\n\")\n",
    "    f.write(f\"k_CV: {k_CV}\\n\")\n",
    "    f.write(f\"n_blocks: {n_blocks}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "def process_session_wrapper(params):\n",
    "    \"\"\"\n",
    "    Wrapper function to catch and handle exceptions from process_session\n",
    "    \"\"\"\n",
    "    mouse, session = params[0], params[1]\n",
    "    try:\n",
    "        print(f\"Starting processing {mouse}/{session}\")\n",
    "        result = process_session(params)\n",
    "        return (mouse, session, \"Success\", result)\n",
    "    except Exception as e:\n",
    "        # Log the error and return it with the mouse/session info\n",
    "        error_msg = f\"Error processing {mouse}/{session}: {str(e)}\"\n",
    "        print(error_msg)\n",
    "        return (mouse, session, \"Failed\", error_msg)\n",
    "\n",
    "\n",
    "def run_parallel_processing():\n",
    "\n",
    "    # Build the task list\n",
    "    all_tasks = []\n",
    "    for mouse in mice:\n",
    "        spike_sessions = os.listdir(os.path.join(spike_dir, mouse))\n",
    "        for session in spike_sessions:\n",
    "            if session in sessions:\n",
    "                # Building the task list\n",
    "                kilosort_dir = os.path.join(spike_dir, mouse, session)\n",
    "                behavior_dir = os.path.join(events_dir, mouse, session)\n",
    "                if not os.path.exists(kilosort_dir) or not os.path.exists(behavior_dir):\n",
    "                    print(f\"Skipping {mouse}/{session} due to missing data.\")\n",
    "                    continue\n",
    "                params = [mouse, session, kilosort_dir, behavior_dir, save_dir, window_size, step_size, fs, sigma_smooth, use_units, speed_threshold, k_CV, n_blocks, n_shifts, plot_predictions]\n",
    "                all_tasks.append(params)\n",
    "\n",
    "    print(f\"Starting threaded processing with {max_workers} workers for {len(all_tasks)} tasks\")\n",
    "\n",
    "    # Create a results log file\n",
    "    results_log_path = os.path.join(save_dir, 'processing_results.txt')\n",
    "    with open(results_log_path, 'w') as f:\n",
    "        f.write(f\"Processing started at: {time.strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "    \n",
    "    # Process in parallel with progress bar\n",
    "    results = []\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit all tasks\n",
    "        future_to_params = {executor.submit(process_session_wrapper, params): params for params in all_tasks}\n",
    "\n",
    "        for future in as_completed(future_to_params):\n",
    "            params = future_to_params[future]\n",
    "            mouse, session = params[0], params[1]\n",
    "            try:\n",
    "                result = future.result()\n",
    "                results.append(result)\n",
    "                status = result[2]\n",
    "                # Log individual completion\n",
    "                with open(results_log_path, 'a') as log_file:\n",
    "                    log_file.write(f\"{mouse}/{session}: {status}\\n\")\n",
    "                    if status == \"Failed\":\n",
    "                        log_file.write(f\"  Error: {result[3]}\\n\")\n",
    "                print(f\"Completed {mouse}/{session} with status: {status}\")\n",
    "            except Exception as exc:\n",
    "                print(f\"Task for {mouse}/{session} generated an exception: {exc}\")\n",
    "                with open(results_log_path, 'a') as log_file:\n",
    "                    log_file.write(f\"{mouse}/{session}: Exception in executor\\n\")\n",
    "                    log_file.write(f\"  Error: {str(exc)}\\n\")\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    # Log summary statistics\n",
    "    success_count = sum(1 for r in results if r[2] == \"Success\")\n",
    "    failed_count = sum(1 for r in results if r[2] == \"Failed\")\n",
    "    \n",
    "    with open(results_log_path, 'a') as log_file:\n",
    "        log_file.write(\"\\n--- SUMMARY ---\\n\")\n",
    "        log_file.write(f\"Total tasks: {len(all_tasks)}\\n\")\n",
    "        log_file.write(f\"Successful: {success_count}\\n\")\n",
    "        log_file.write(f\"Failed: {failed_count}\\n\")\n",
    "        log_file.write(f\"Total elapsed time: {elapsed_time:.2f} seconds\\n\")\n",
    "    \n",
    "    print(f\"Processing complete! Results saved to {results_log_path}\")\n",
    "    print(f\"Successful: {success_count}, Failed: {failed_count}\")\n",
    "    print(f\"Total elapsed time: {elapsed_time:.2f} seconds\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results = run_parallel_processing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collecting and plotting all the results from the decoding analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect decoding results\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import ranksums\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "decoding_dir = r\"E:\\clickbait-ephys\\figures\\Decoding\\Kalman_filter (2-27-25)\\window_size_0.1_step_size_0.1_sigma_2.5_n_shifts_10\"\n",
    "mice = ['6002', '6003', '6000', '6001']\n",
    "regions = ['OB', 'HC']\n",
    "\n",
    "alpha = 0.05\n",
    "results = []\n",
    "\n",
    "for mouse in mice:\n",
    "    mouse_dir = os.path.join(decoding_dir, mouse)\n",
    "    sessions = os.listdir(mouse_dir)\n",
    "    for session in sessions:\n",
    "        for region in regions:\n",
    "\n",
    "            null_rmse_file = os.path.join(mouse_dir, session, region, 'null_rmse.npy')\n",
    "            true_rmse_file = os.path.join(mouse_dir, session, region, 'true_rmse.npy')\n",
    "            if not os.path.exists(null_rmse_file) or not os.path.exists(true_rmse_file):\n",
    "                continue\n",
    "            null_rmse = np.load(null_rmse_file)\n",
    "            true_rmse = np.load(true_rmse_file)\n",
    "\n",
    "            # rank sum test\n",
    "            _ , p_val = ranksums(true_rmse, null_rmse, 'less')\n",
    "            results.append({'mouse': mouse, 'session': session, 'region': region, 'p_val': p_val})\n",
    "\n",
    "# convert to dataframe\n",
    "results = pd.DataFrame(results)\n",
    "\n",
    "# save the results\n",
    "results.to_csv(os.path.join(decoding_dir, 'results.csv'), index=False)\n",
    "print(f'Number of significant sessions: \\nHippocampus: {np.sum(results[results.region == \"HC\"].p_val < alpha)}\\nOlfactory Bulb: {np.sum(results[results.region == \"OB\"].p_val < alpha)}')\n",
    "\n",
    "\n",
    "# Create a connected points plot\n",
    "# First, pivot the data to create pairs\n",
    "# Create a unique identifier for each mouse-session pair\n",
    "results['mouse_session'] = results['mouse'] + '_' + results['session']\n",
    "\n",
    "# Create a pivot table where each row is a mouse-session pair and columns are regions\n",
    "pivot_df = results.pivot(index='mouse_session', columns='region', values='p_val')\n",
    "\n",
    "# Extract unique mouse-session pairs that have both regions\n",
    "complete_pairs = pivot_df.dropna().index\n",
    "\n",
    "# Create a figure\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Generate jitter positions for each mouse-session and region\n",
    "jitter_positions = {}\n",
    "for idx in complete_pairs:\n",
    "    jitter_positions[idx] = {}\n",
    "    for region in regions:\n",
    "        jitter_positions[idx][region] = np.random.uniform(-0.1, 0.1)\n",
    "\n",
    "# Plot connections first\n",
    "for idx in complete_pairs:\n",
    "    # Get the p-values for both regions for this mouse-session\n",
    "    ob_pval = pivot_df.loc[idx, 'OB']\n",
    "    hc_pval = pivot_df.loc[idx, 'HC']\n",
    "    \n",
    "    # Get the jittered x-positions\n",
    "    ob_x = 0 + jitter_positions[idx]['OB']\n",
    "    hc_x = 1 + jitter_positions[idx]['HC']\n",
    "    \n",
    "    # Plot the connecting line\n",
    "    plt.plot([ob_x, hc_x], [ob_pval, hc_pval], '-', color='gray', alpha=0.5, zorder=1)\n",
    "\n",
    "# Create a color map for mice\n",
    "mouse_colors = {mouse: plt.cm.tab10(i) for i, mouse in enumerate(mice)}\n",
    "\n",
    "# Now plot individual points with mouse-specific colors\n",
    "for idx in complete_pairs:\n",
    "    mouse, session = idx.split('_', 1)\n",
    "    \n",
    "    # Get the p-values for both regions\n",
    "    ob_pval = pivot_df.loc[idx, 'OB']\n",
    "    hc_pval = pivot_df.loc[idx, 'HC']\n",
    "    \n",
    "    # Get the jittered x-positions (same as used for the lines)\n",
    "    ob_x = 0 + jitter_positions[idx]['OB']\n",
    "    hc_x = 1 + jitter_positions[idx]['HC']\n",
    "    \n",
    "    # Plot points for OB\n",
    "    plt.scatter(ob_x, ob_pval, color=mouse_colors[mouse], \n",
    "                label=f'Mouse {mouse}' if idx == complete_pairs[0] or mouse not in [p.split('_')[0] for p in complete_pairs[:complete_pairs.get_loc(idx)]] else \"\",\n",
    "                alpha=0.7, s=50, zorder=2)\n",
    "    \n",
    "    # Plot points for HC\n",
    "    plt.scatter(hc_x, hc_pval, color=mouse_colors[mouse], \n",
    "                alpha=0.7, s=50, zorder=2)\n",
    "\n",
    "# Add threshold line\n",
    "plt.axhline(y=alpha, color='r', linestyle='--', label=f'α = {alpha}')\n",
    "\n",
    "# Label and customize\n",
    "plt.xticks([0, 1], ['OB', 'HC'])\n",
    "plt.ylabel('p-value')\n",
    "plt.title('Paired p-values for decoding results by region')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(decoding_dir, 'paired_p_values_jittered.png'), dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Create an additional plot showing average by mouse\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Group by mouse and region to get average p-values\n",
    "mouse_avg = results.groupby(['mouse', 'region'])['p_val'].median().reset_index()\n",
    "mouse_avg_pivot = mouse_avg.pivot(index='mouse', columns='region', values='p_val')\n",
    "\n",
    "# Plot average lines for each mouse\n",
    "for mouse in mice:\n",
    "    if mouse in mouse_avg_pivot.index and not mouse_avg_pivot.loc[mouse].isna().any():\n",
    "        plt.plot([0, 1], \n",
    "                 [mouse_avg_pivot.loc[mouse, 'OB'], mouse_avg_pivot.loc[mouse, 'HC']], \n",
    "                 'o-', label=f'Mouse {mouse}', \n",
    "                 color=mouse_colors[mouse], \n",
    "                 linewidth=2, \n",
    "                 markersize=8)\n",
    "\n",
    "# Add threshold line\n",
    "plt.axhline(y=alpha, color='r', linestyle='--', label=f'α = {alpha}')\n",
    "\n",
    "# Label and customize\n",
    "plt.xticks([0, 1], ['OB', 'HC'])\n",
    "plt.ylabel('Median p-value')\n",
    "plt.title('Median p-values by mouse and region')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(decoding_dir, 'avg_p_values_by_mouse.png'), dpi=300)\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
